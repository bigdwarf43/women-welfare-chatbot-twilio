{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46017ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\python\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "#For tensorflow model\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12b0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the intents file\n",
    "\n",
    "import json\n",
    "\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853a4544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314 documents\n",
      "47 classes ['Women competitions', 'Women hackathons', 'anxiety', 'bad', 'body image', 'breast cancer', 'bullying', 'can I', 'coping', 'counsellor', 'cure', 'depression', 'feeling', 'funding and financing', 'gender bias and discrimination', 'good', 'goodbye', 'government schemes', 'greeting', 'hygiene', 'imposter syndrome', 'lonely', 'menopause', 'mensuration problems', 'mental illness', 'mood', 'networking and mentorship', 'peer pressure', 'periods', 'pregnancy', 'premenstrual syndrome', 'psychiatrist', 'psychologist', 'relationships', 'scholarships for women', 'self-esteem', 'sexually transmitted infections', 'stress', 'thanks', 'treatment', 'vaginal infection', 'women career opportunities', 'women in military', \"women's colleges and scholarships\", \"women's sports\", 'work life balance', 'working women in India']\n",
      "496 unique stemmed words ['!', \"'m\", \"'s\", ',', '.', '5g', ':', 'a', 'about', 'acc', 'adv', 'affect', 'afraid', 'aft', 'ag', 'agit', 'agr', 'aim', 'al', 'allow', 'alon', 'although', 'alway', 'am', 'an', 'and', 'anxy', 'any', 'anyon', 'appear', 'ar', 'are', 'arrang', 'as', 'assault', 'at', 'athlet', 'attack', 'attend', 'aug', 'av', 'avail', 'avoid', 'bachao', 'bad', 'bal', 'bas', 'bath', 'be', 'becaus', 'becom', 'being', 'benefit', 'bet', 'between', 'bia', 'biggest', 'blockchain', 'body', 'both', 'breakdown', 'breast', 'bring', 'brush', 'bul', 'burnout', 'busy', 'by', 'bye', 'ca', 'can', 'cant', 'car', 'caught', 'caus', 'cent', 'ceo', 'challeng', 'chang', 'childbir', 'civ', 'classm', 'clin', 'cod', 'coeduc', 'col', 'colleg', 'comb', 'common', 'comp', 'company', 'competit', 'comply', 'confid', 'connect', 'consid', 'const', 'cont', 'control', 'cop', 'counsel', 'country', 'cours', 'cov', 'cramp', 'cre', 'crush', 'cur', 'cybersec', 'cyst', 'dat', 'dea', 'degr', 'depress', 'detect', 'develop', 'diet', 'difficult', 'direct', 'disappoint', 'discrimin', 'divers', 'do', 'doct', 'doe', 'doing', 'dont', 'donâ€™t', 'down', 'dur', 'e-haat', 'ear', 'easy', 'edg', 'educ', 'effect', 'emot', 'empow', 'enco', 'energy', 'engross', 'enjoy', 'enough', 'enterpr', 'entrepr', 'environ', 'espec', 'ev', 'evolv', 'exclud', 'exhaust', 'expect', 'expend', 'expery', 'explain', 'fac', 'fact', 'fail', 'fam', 'fash', 'feel', 'find', 'firm', 'flex', 'for', 'forget', 'friend', 'from', 'fund', 'fung', 'gap', 'gend', 'germ', 'get', 'girl', 'go', 'good', 'goodby', 'got', 'govern', 'grant', 'gre', 'greh', 'group', 'guid', 'hackathon', 'hair', 'hand', 'hap', 'happy', 'harass', 'has', 'hav', 'having/fac', 'head', 'heal', 'healthy', 'hello', 'help', 'hey', 'hi', 'high', 'hir', 'hom', 'hopeless', 'hormon', 'hostelit', 'how', 'hygy', 'i', 'if', 'il', 'imb', 'impact', 'import', 'impost', 'improv', 'in', 'incom', 'ind', 'industry', 'infect', 'inform', 'inpaty', 'int', 'intern', 'invest', 'involv', 'is', 'it', 'ix', 'iâ€™ve', 'job', 'joy', 'judg', 'kendr', 'know', 'last', 'lat', 'lead', 'learn', 'leav', 'left', 'leg', 'lic', 'lif', 'lik', 'list', 'littl', 'loc', 'lon', 'long', 'look', 'los', 'lot', 'mahil', 'maintain', 'mak', 'mammogram', 'man', 'mantr', 'many', 'matern', 'matru', 'me', 'mean', 'med', 'men', 'menopaus', 'menst', 'ment', 'metast', 'milit', 'minim', 'misconceiv', 'miss', 'mor', 'mot', 'moth', 'much', 'my', 'myself', \"n't\", 'nat', 'nee', 'network', 'nev', 'new', 'night', 'no', 'nobody', 'not', 'nutrit', 'occ', 'occup', 'od', 'of', 'off', 'oft', 'on', 'onlin', 'opportun', 'opt', 'or', 'org', 'oth', 'out', 'ov', 'overwhelm', 'own', 'padhao', 'pain', 'pan', 'pap', 'part-time', 'particip', 'pay', 'perc', 'period', 'person', 'personnel', 'phys', 'pick', 'plac', 'play', 'pleas', 'pms', 'popul', 'posit', 'postpart', 'pract', 'pradh', 'pregn', 'premenst', 'prep', 'press', 'prev', 'priorit', 'problem', 'program', 'provid', 'psycholog', 'psychy', 'ptsd', 'purpos', 'pursu', 'q', 'qual', 'rais', 'real', 'rec', 'receiv', 'reconstruct', 'recov', 'reduc', 'reject', 'relax', 'reliev', 'remedy', 'repres', 'resourc', 'retir', 'risk', 'rol', 'sad', 'saf', 'sal', 'scheme', 'schemes', 'scholarships', 'school', 'sci', 'second', 'see', 'seek', 'self-doubt', 'self-doubting', 'self-harm', 'serv', 'sex', 'shakt', 'shar', 'should', 'show', 'sid', 'sign', 'sleep', 'smear', 'so', 'soc', 'socy', 'som', 'spac', 'spec', 'sport', 'spread', 'start', 'stat', 'stem', 'step', 'sti', 'stil', 'stop', 'strategies', 'strengths', 'struggling', 'study', 'suicid', 'support', 'sur', 'surgery', 'suspicy', 'swadh', 'symptom', 'syndrom', 'tak', 'talk', 'teas', 'tech', 'technolog', 'tee', 'test', 'than', 'thank', 'that', 'the', 'their', 'ther', 'therapy', 'thes', 'thi', 'thing', 'think', 'thought', 'through', 'tim', 'tir', 'titl', 'to', 'today', 'too', 'trailblaz', 'train', 'transmit', 'transport', 'tre', 'trend', 'tri', 'troubl', 'try', 'typ', 'ujjawal', 'undergradu', 'unhappy', 'unit', 'unsec', 'up', 'us', 'vagin', 'vandan', 'very', 'vet', 'virt', 'voc', 'wag', 'want', 'warn', 'wash', 'way', 'we', 'weak', 'websit', 'wel', 'wellâ€', 'what', 'when', 'wher', 'which', 'whil', 'who', 'why', 'with', 'wom', 'women-led', 'women-owned', 'work', 'work-from-home', 'work-life', 'workplac', 'world', 'worldwid', 'worry', 'worthless', 'yojan', 'you', '“']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "#looping through every intent\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenizing each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        #add to the wordList\n",
    "        words.extend(w)\n",
    "        \n",
    "        #add to documents\n",
    "        documents.append((w, intent['tag']))\n",
    "        \n",
    "        #add to classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "            \n",
    "#stem and lower each word and removing duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0106fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashw\\AppData\\Local\\Temp\\ipykernel_15696\\987945108.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "#create empty array for output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    #create bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202a732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x example: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1] \n",
    "# train_y example: [0, 0, 1, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f253495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 79999  | total loss: \u001b[1m\u001b[32m0.02809\u001b[0m\u001b[0m | time: 0.145s\n",
      "| Adam | epoch: 2000 | loss: 0.02809 - acc: 0.9752 -- iter: 312/314\n",
      "Training Step: 80000  | total loss: \u001b[1m\u001b[32m0.02529\u001b[0m\u001b[0m | time: 0.150s\n",
      "| Adam | epoch: 2000 | loss: 0.02529 - acc: 0.9777 -- iter: 314/314\n",
      "--\n",
      "INFO:tensorflow:D:\\study\\sem5\\nlp\\contextBasedChatbot\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=2000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a090d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea68d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06a2803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\study\\sem5\\nlp\\contextBasedChatbot\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e52dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b714f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"results\",results)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    print(\"return_list\", return_list)\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return print(random.choice(i['responses']))\n",
    "                    else:\n",
    "                        print(\"I do not understand\")\n",
    "    \n",
    "            results.pop(0)\n",
    "    else:\n",
    "        print(\"I do not understand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b27a1181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [[6, 0.81899565]]\n",
      "return_list [('bullying', 0.81899565)]\n",
      "There are resources available to help with bullying, including hotlines and online support groups. It's important to take action and get the help you need to feel safe and supported.\n"
     ]
    }
   ],
   "source": [
    "response('My parents dont support me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "499e9333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [[25, 0.51082295], [3, 0.479516]]\n",
      "return_list [('mood', 0.51082295), ('bad', 0.479516)]\n",
      "I'm sorry to hear that. Can you tell me more about what's going on?\n"
     ]
    }
   ],
   "source": [
    "response(\"i feel depressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146a7b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [[35, 0.99908864]]\n",
      "return_list [('self-esteem', 0.99908864)]\n",
      "It can be helpful to challenge negative self-talk by writing down your accomplishments and positive traits. You can also try talking to a trusted friend or counselor about how you're feeling.\n"
     ]
    }
   ],
   "source": [
    "response(\"My friends don't like me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2efb17b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results [[25, 0.9924614]]\n",
      "return_list [('mood', 0.9924614)]\n",
      "It can be tough to feel sad. Would you like to talk about what's been bothering you?\n"
     ]
    }
   ],
   "source": [
    "response(\"i am going to have a baby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887ce53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
